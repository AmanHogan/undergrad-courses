{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression - using labels to predict a **continuous** output\n",
    "\n",
    "### Types of regression\n",
    "- Linear regression: **zz** linear relation ($y = mx + b$)\n",
    "- Polynomial regression: curved relation ($y = mx_2 + b$)\n",
    "- Multiple Linear Regression: more than one feature/independent variable ($y = w_1x_1 + w_2x_2 + b$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Vector\n",
    "\n",
    "Given you have $n$ features, here is one sample in the data set, where $i$ is the index of the sample:\n",
    "\n",
    "$x^{(i)} = [x_{(1)}^{(i)}, x_{(2)}^{(i)}, x_{(3)}^{(i)}, ..., x_{(n)}^{(i)}] $\n",
    "\n",
    "Make a design feature vector by adding a 1 to the begining of the vector to account for the bias b:\n",
    "\n",
    "$x^{(i)} = [1^{(i)}, x_{(1)}^{(i)}, x_{(2)}^{(i)}, x_{(3)}^{(i)}, ..., x_{(n)}] $ \n",
    "\n",
    "Now create a weight vector that is size $n + 1$:\n",
    "\n",
    "$w = [w_{(1)}, w_{(2)}]$\n",
    "\n",
    "### Hypothesis \n",
    "\n",
    "Your prediction or hypothesis is $h(x;w) = w_T \\cdot x_{i}$\n",
    "\n",
    "$w_T$ is the transposed weight matrix and $x_{i}$ is the feature vector at $i$:\n",
    "\n",
    "$h(x;w) = [w_{(1)}, w_{(2)}, ...,  w_{(n+1)}] \\cdot \\begin{pmatrix} \\\\1^{(i)} \\\\x_{(1)}^{(i)} \\\\... \\\\x_{(n)}^{(i)}\\end{pmatrix}$\n",
    "\n",
    "which would yield:\n",
    "\n",
    "$h(x;w) = w_{(1)}(1)^{(i)} + w_{(2)}x_{(1)}^{(i)} + ... + w_{(n+1)}x_{(n)}^{(i)} $\n",
    "\n",
    "\n",
    "Since $w_{(1)}(1)^{(i)}$ is the same as $w_{(1)}$ we can just call that $b$ or the bias.\n",
    "\n",
    "\n",
    "### Measuring our Regression\n",
    "We can use Mean Squared Error to see how good our prediction is compared to the actual data.\n",
    "\n",
    "We create a **cost/loss** function:\n",
    "\n",
    "$J(w) = \\frac{1}{2} \\sum_{i=1}^{m} (h(x^i;w) - y^i)^2$\n",
    "\n",
    "\n",
    "where $i$ is the index of the training sample, $m$ is the number of samples, $y$ is the label, and $x$ is the feature vector, and $h(x^i;w)$ is the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "We are incrementally changing the vector $w_{j+1}$'s values to minimize the cost function ($J(w)$). We do this by taking the derrivitive of the cost fucntion $\\nabla J(w)$, scaling it by a learning rate ($\\alpha$) and subtracting the obtained value from the current $w_{j}$.\n",
    "\n",
    "## Batch Gradient Descent Algorithm\n",
    "- Goes over entire dataset each iteration and calculates gradient.\n",
    "- Upside: Slow\n",
    "- Downside: Converges\n",
    "\n",
    "**Until Convergence:**\n",
    "- $w_{j} := w_{j} - \\alpha (\\frac{1}{m} \\sum_{i=1}^{m} (h(x^i;w) - y^i) x^{(i)}) $\n",
    "\n",
    "## Stochsatic Gradient Descent\n",
    "- Chooses random sample each iteration.\n",
    "- Upside: Faster\n",
    "- Downside: Never fully converges\n",
    "\n",
    "**Until Convergence:**\n",
    "- for i = 1 to m\n",
    "    - Choose random x-y sample\n",
    "    - $w_{j} := w_{j} - \\alpha (h(x^i;w) - y^i) x^{(i)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Equations\n",
    "\n",
    "- Directly computes the parameters of a model that minimizes the Sum\n",
    "of the squared difference between the actual term and the predicted\n",
    "term\n",
    "- Can only be used for linear regression\n",
    "- Assumes the $X$ is a feature matrix that includes the bias term\n",
    "\n",
    "$W = (X^{T}X)^-1 \\cdot (X^{T}y)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "data =  pd.read_csv('lreg.csv')\n",
    "\n",
    "x = np.insert(np.array(data['studytime']).reshape(len(data['studytime']), 1), 0, 1, axis=1)\n",
    "y = np.array(data['score']).reshape(len(data['score']), 1)\n",
    "w = np.ones(2).reshape(2,1)\n",
    "m =len(data['studytime'])\n",
    "epochs = 1000\n",
    "alpha = .001\n",
    "\n",
    "loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(m):\n",
    "        index = np.random.randint(m)\n",
    "        for j in range(len(w)):\n",
    "            w[j] = (w[j] - alpha * (np.dot(w.T, x[index]) - y[index]) * x[index][j])\n",
    "\n",
    "    loss.append(((np.dot(w.T, x[index]) - y[index])**2) / 2 )\n",
    "\n",
    "\n",
    "plt.plot(data[\"studytime\"], data['score'], 'o', label='Actual')  # Plot the data points\n",
    "plt.title('Stochastic Linear Regression for Test Scores')\n",
    "plt.xlabel('Study Time')\n",
    "plt.ylabel('Score')\n",
    "plt.plot(data[\"studytime\"], np.dot(x, w), '-', label='Prediction')  # Plot the linear regression line\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "plt.title('Loss over Epochs (Stohastic)')\n",
    "plt.plot(*range(epochs), loss)  # Plot the data points\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "data =  pd.read_csv('lreg.csv')\n",
    "\n",
    "x = np.insert(np.array(data['studytime']).reshape(len(data['studytime']), 1), 0, 1, axis=1)\n",
    "y = np.array(data['score']).reshape(len(data['score']), 1)\n",
    "w = np.ones(2).reshape(2,1)\n",
    "m =len(data['studytime'])\n",
    "epochs = 100\n",
    "alpha = .001\n",
    "\n",
    "loss = []\n",
    "\n",
    "def gradient(w,x,y,j):\n",
    "    grad = 0\n",
    "    for i in range(m):\n",
    "        grad += (np.dot(w.T, x[i]) - y[i]) * x[i][j]\n",
    "    return grad\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(m):\n",
    "        index = np.random.randint(m)\n",
    "        for j in range(len(w)):\n",
    "            w[j] = (w[j] - alpha * (1/m) * gradient(w,x,y,j))\n",
    "\n",
    "    loss.append( (1/m) * ((np.dot(w.T, x[index]) - y[index])**2) / 2 )\n",
    "\n",
    "\n",
    "plt.plot(data[\"studytime\"], data['score'], 'o', label='Actual')  # Plot the data points\n",
    "plt.title('Stochastic Linear Regression for Test Scores')\n",
    "plt.xlabel('Study Time')\n",
    "plt.ylabel('Score')\n",
    "plt.plot(data[\"studytime\"], np.dot(x, w), '-', label='Prediction')  # Plot the linear regression line\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "plt.title('Loss over Epochs (Stohastic)')\n",
    "plt.plot(*range(epochs), loss)  # Plot the data points\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

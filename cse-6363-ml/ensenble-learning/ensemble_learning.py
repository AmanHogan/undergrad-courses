# -*- coding: utf-8 -*-
"""ensemble_learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_fRks5B_Of4m35scnD7Wl35cVMz22tiB
"""

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingRegressor
from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import load_iris


def ensemble_learning(X, y, task='classification'):
    """
    Implement ensemble learning methods: Decision Trees, Bagging, Random Forest, and Boosting.

    Parameters:
    - X: Input features (numpy array or pandas DataFrame)
    - y: Target variable (numpy array or pandas Series)
    - task: Type of task, either 'classification' or 'regression' (default: 'classification')

    Returns:
    - results: Dictionary containing evaluation results for each ensemble method
    """
    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    # Initialize results dictionary
    results = {}

    # Decision Tree
    dt_classifier = DecisionTreeClassifier()
    dt_classifier.fit(X_train, y_train)
    dt_predictions = dt_classifier.predict(X_test)

    # Calculate accuracy
    dt_accuracy = accuracy_score(y_test, dt_predictions)
    results['Decision Tree'] = dt_accuracy

    # Bagging
    bagging_classifier = BaggingClassifier(estimator=dt_classifier, n_estimators=2, random_state=42)
    bagging_classifier.fit(X_train, y_train)
    bagging_predictions = bagging_classifier.predict(X_test)

    # Calculate accuracy
    bagging_accuracy = accuracy_score(y_test, bagging_predictions)
    results['Bagging'] = bagging_accuracy

    # Random Forest
    rf_classifier = RandomForestClassifier(n_estimators=2, random_state=42)
    rf_classifier.fit(X_train, y_train)
    rf_predictions = rf_classifier.predict(X_test)

    # Calculate accuracy
    rf_accuracy = accuracy_score(y_test, rf_predictions)
    results['Random Forest'] = rf_accuracy
    
    if task == 'classification':

        # Boosting ADA
        boosting_classifier = AdaBoostClassifier(estimator=DecisionTreeClassifier(), n_estimators=2, random_state=42)
        boosting_classifier.fit(X_train, y_train)
        boosting_predictions = boosting_classifier.predict(X_test)

        # Calculate accuracy
        boosting_accuracy = accuracy_score(y_test, boosting_predictions)
        results['Boosting'] = boosting_accuracy

    else:
        # Boosting GBR
        # For regression tasks, you can use GradientBoostingRegressor instead
        gbr_classifier = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
        gbr_classifier.fit(X_train, y_train)
        gbr_predictions = gbr_classifier.predict(X_test)

        # Calculate accuracy
        gbr_mse = mean_squared_error(y_test, gbr_predictions)
        results['Boosting GBR'] = gbr_mse

    # Visualization
    labels = list(results.keys())
    values = list(results.values())

    plt.figure(figsize=(10, 6))
    if task == 'classification':
        plt.bar(labels, values, color=['blue', 'orange', 'green', 'red'])
        plt.ylabel('Accuracy')
    else:
        plt.bar(labels, values, color='purple')
        plt.ylabel('Mean Squared Error')
    plt.title('Ensemble Learning Methods Visualization')
    plt.show()

    return results

# Example usage:
iris = load_iris()
X = iris.data
y = iris.target
results = ensemble_learning(X, y, task='classification')
print(results)